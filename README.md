
### Data Structure  
```  
AudioSpoof/  
├── metadata/  
│   └── SPKINFO.txt                  # Speaker metadata  
├── wav/  
│   ├── dev/                         # Original development set audio  
│   │   ├── {speaker_id}/            # e.g., 5_541/  
│   │   │   └── {utterance_id}.wav   # e.g., 5_541_20170607142131.wav  
│   ├── test/                        # Original test set audio (same structure as dev)  
│   ├── train/                       # Original training set audio (same structure as dev)  
│   │  
│   ├── dev-naturalspeech3/          # Voice cloning results (Model 1)  
│   ├── test-naturalspeech3/         # (Structure consistent with the original wav directory)  
│   ├── train-naturalspeech3/  
│   │  
│   ├── dev-cosyvoice/               # Voice cloning results (Model 2)  
│   ├── test-cosyvoice/  
│   ├── train-cosyvoice/  
│   │  
│   ├── dev-F5TTS/                   # Voice cloning results (Model 3)  
│   ├── test-F5TTS/  
│   ├── train-F5TTS/  
│   │  
│   └── dev-sparktts/                # Voice cloning results (Model 4)  
│   ├── test-sparktts/  
│   └── train-sparktts/  
└── text/  
    ├── dev.txt                      # Development set text (format: path|text)  
    ├── test.txt                     # Test set text  
    └── train.txt                    # Training set text  
```  

### Key Notes:  
1. The three directories (dev/test/train) generated by each cloning model maintain the structure of the original audio directory.  
2. Voice cloning directory names follow the format `{subset}-{model_name}` (e.g., `dev-naturalspeech3`).  
3. Cloned audio files retain the original naming (e.g., `5_541_20170607142131.wav`), with different model outputs distinguished only by directory paths.  

Four models were used for zero-shot voice cloning of each speech clip, with results stored in the `wav` folder. Directory names include the corresponding model suffixes:  
1. naturalspeech3  
2. cosyvoice  
3. F5-TTS  
4. spark-tts  


### Magicdata-mini Dataset (Human Voice Dataset)  
AudioSpoof is based on the [Magicdata](https://www.magicdatatech.cn/) dataset. It randomly samples 10% of speakers from the dev and test subsets, and 2% of speakers from the train subset. Each selected speaker contributes 10% of their recordings.  

| Subset | Number of Speakers | Number of Audio Clips |
| ------ | ------------------ | --------------------- |
| dev    | 2                  | 118                   |
| test   | 4                  | 180                   |
| train  | 20                 | 1105                  |

**Total:** 26 speakers, 1403 audio clips.  

These sampled data were then cloned using the four models above.